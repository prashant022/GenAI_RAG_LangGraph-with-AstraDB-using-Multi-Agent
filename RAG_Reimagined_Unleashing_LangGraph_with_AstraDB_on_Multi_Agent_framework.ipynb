{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Here we build a RAG using LangGraph using Astra DB from Datasax . This would be a multi agent framework architecture as shown.The architecture uses Llama 3.1 LLM as the Gen ai model via GROQ ![mmCapture.JPG](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "Nu802E7ohvc7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "937MX0I4of8U",
        "outputId": "ba27420a-7b41-495a-9a1f-2ad96227d675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.28-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting cassio\n",
            "  Downloading cassio-0.1.9-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.6 (from langchain)\n",
            "  Downloading langchain_core-0.3.6-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.129-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting langgraph-checkpoint<2.0.0,>=1.0.2 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-1.0.12-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting cassandra-driver<4.0.0,>=3.28.0 (from cassio)\n",
            "  Downloading cassandra_driver-3.29.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Collecting geomet<0.3,>=0.1 (from cassandra-driver<4.0.0,>=3.28.0->cassio)\n",
            "  Downloading geomet-0.2.1.post1-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.6->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (4.12.2)\n",
            "Collecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<2.0.0,>=1.0.2->langgraph)\n",
            "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver<4.0.0,>=3.28.0->cassio) (8.1.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver<4.0.0,>=3.28.0->cassio) (1.16.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.3.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.2.28-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cassio-0.1.9-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cassandra_driver-3.29.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.6-py3-none-any.whl (399 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_checkpoint-1.0.12-py3-none-any.whl (17 kB)\n",
            "Downloading langsmith-0.1.129-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.2/292.2 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading geomet-0.2.1.post1-py3-none-any.whl (18 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, msgpack, jsonpointer, h11, geomet, jsonpatch, httpcore, cassandra-driver, httpx, cassio, langsmith, langchain-core, langgraph-checkpoint, langchain-text-splitters, langgraph, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.0.8\n",
            "    Uninstalling msgpack-1.0.8:\n",
            "      Successfully uninstalled msgpack-1.0.8\n",
            "Successfully installed cassandra-driver-3.29.2 cassio-0.1.9 geomet-0.2.1.post1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.1 langchain-core-0.3.6 langchain-text-splitters-0.3.0 langgraph-0.2.28 langgraph-checkpoint-1.0.12 langsmith-0.1.129 msgpack-1.1.0 orjson-3.10.7 tenacity-8.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langgraph cassio\n",
        "# Cassio is a separate open-source package that provides additional\n",
        "# functionality for working with Apache Cassandra %%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the connection to astra DB and initialize it now !!"
      ],
      "metadata": {
        "id": "rYUiLOynr379"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cassio\n",
        "ASTRA_DB_APPLICATION_TOKEN = \"AstraCS:HDgqLtYZiRWIOJJINwILZBRj:ce3cd8eff5bd2ccdc35012020f1986eb1f4a24ac6074cdb1325f91ac25360411\"\n",
        "ASTRA_DB_ID=\"9b3de0cd-8279-4b34-b7db-2c159d7d02f8\"\n",
        "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN,database_id=ASTRA_DB_ID)"
      ],
      "metadata": {
        "id": "y69aNYJzrDEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMhF1BajtAxd",
        "outputId": "e36d88b3-8aeb-4739-e509-1efdd833cba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.1)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.129)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.1->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.1->langchain_community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.7)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain_community) (2.23.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.2)\n",
            "Downloading langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain_community-0.3.1 marshmallow-3.22.0 mypy-extensions-1.0.0 pydantic-settings-2.5.2 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ufADpZg6htI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain_community tiktoken langchain-groq langchainhub langchain langgraph langchain_huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aylxdb1etG6d",
        "outputId": "d9f2fedd-c6c6-4860-c7aa-7ecea13eccf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.1)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.2.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.1)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.10/dist-packages (0.2.28)\n",
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.129)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.5.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (24.1)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: langgraph-checkpoint<2.0.0,>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from langgraph) (1.0.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.24.7)\n",
            "Collecting sentence-transformers>=2.6.0 (from langchain_huggingface)\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.19.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (4.44.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.11.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.27.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (1.33)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<2.0.0,>=1.0.2->langgraph) (1.1.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.5)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.2.0-py3-none-any.whl (14 kB)\n",
            "Downloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
            "Downloading langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: types-requests, tiktoken, langchainhub, groq, sentence-transformers, langchain-groq, langchain_huggingface\n",
            "Successfully installed groq-0.11.0 langchain-groq-0.2.0 langchain_huggingface-0.1.0 langchainhub-0.1.21 sentence-transformers-3.1.1 tiktoken-0.7.0 types-requests-2.32.0.20240914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odP4GPJq5G92",
        "outputId": "8097c18f-376b-4436-df3e-bac55d871f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Docs to index"
      ],
      "metadata": {
        "id": "_U_2JEsldc9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "        \"https://deeprevision.github.io/posts/001-transformer/\",\n",
        "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "        \"https://developer.nvidia.com/blog/nvidia-blackwell-platform-sets-new-llm-inference-records-in-mlperf-inference-v4-1/\"\n",
        "]"
      ],
      "metadata": {
        "id": "0pjeNCIx5Kvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=0)\n",
        "doc_splits = text_splitter.split_documents(docs_list)"
      ],
      "metadata": {
        "id": "mrl18orSdlbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_list"
      ],
      "metadata": {
        "id": "WraRsyo3d17g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#huggingFace embeddings model & Vector store in astra"
      ],
      "metadata": {
        "id": "WJPX4gkkhRIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3SUiEEDfc3U",
        "outputId": "51ba717d-88a7-47ac-efda-37a35419d437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "astra_vector_store=Cassandra(embedding=embeddings,\n",
        "                             table_name ='db221_me_rag',\n",
        "                             session=None,\n",
        "                             keyspace=None)"
      ],
      "metadata": {
        "id": "BlTfLnCwhYpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "astra_vector_store.add_documents(docs_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8zGZgwvjC4v",
        "outputId": "c83df040-d5c5-4bdd-cad8-be7287389066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['90a20be5abb34b658116937060b64fb5',\n",
              " 'c6d255f7cfde48cabc6cf976f53cfe05',\n",
              " '722585caab384c7f8365f3fc887cb021']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Inserted %i headlines.\" % len(doc_splits))\n",
        "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxDpr9A5jswj",
        "outputId": "7f1af635-00ed-4577-d124-8b71dee79d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserted 97 headlines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reterive the data from astra_vector_index to check the data"
      ],
      "metadata": {
        "id": "Nem3sJwVj7Px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=astra_vector_store.as_retriever()\n",
        "retriever.invoke(\"What is transformer\",ConsistencyLevel=\"LOCAL_ONE\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVeFjr8Vj_-6",
        "outputId": "04bdff7d-6835-407c-99ca-0e066b5c5f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'description': 'A deep dive into Transformer, a neural network architecture that was introduced in the famous paper “attention is all you need” in 2017, its applications, impacts, challenges and future directions', 'language': 'en', 'source': 'https://deeprevision.github.io/posts/001-transformer/', 'title': 'AI Research Blog - The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\nAI Research Blog - The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAI Research Blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n\\n\\n\\n\\n\\n\\nThe Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture\\n\\n\\n          A deep dive into Transformer, a neural network architecture that was introduced in the famous paper “attention is all you need” in 2017, its applications, impacts, challenges and future directions\\n        \\n\\n\\ntransformers\\nneural architectures\\nNLP\\ncomputer vision\\ndeep learning\\n\\n\\n\\n\\nAuthor\\nAffiliation\\n\\nJean Nyandwi \\n\\n\\n\\n              Carnegie Mellon University\\n            \\n\\n\\n\\n\\nPublished\\n\\nJuly 29, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\nOn this page\\n\\nIntroduction\\nNeural Networks Before Transformers\\n\\nMultiLayer Perceptrons(MLPs)\\nConvolutional Neural networks\\nRecurrent Neural Networks\\n\\nTransformer Architecture\\n\\nEncoder\\nDecoder\\nAttention\\n\\nWhat Really is Attention?\\nAttention Function: Query, Key, Value\\nMulti-Head Attention\\n\\nMLPs\\nEmbeddings and Positional Encoding Layers\\nResidual Connections, Layer Normalization, and Dropout\\nLinear and Softmax Layers\\n\\nVisualizing Attention\\nThe Pros and Cons of Attention\\nLarge Language Transformer Models\\n\\nEvolution of LLMs\\nEncoder, Decoder, Encoder-decoder LLMs\\n\\nEncoder-only LLMs\\nDecoder-only LLMs\\nEncoder-Decoder LLMs\\n\\nVertical LLMs\\n\\nTransformers Beyond NLP: Vision and other Modalities\\nTransformer: Current Challenges and Future Directions\\n\\nEfficient Transformers\\nTransformers with Effective Long Contexts\\nMultimodal Transformer\\n\\nOpen-source Implementations of Transformer\\nSupplementary Resources\\nConclusion\\nReferences\\n\\n\\n\\n\\n\\n\\n\\nIntroduction\\nInvented in 2017 and first presented in the ground-breaking paper “Attention is All You Need”(Vaswani et al. 2017), the transformer model has been a revolutionary contribution to deep learning and arguably, to computer science as a whole. Born as a tool for neural machine translation, it has proven to be far-reaching, extending its applicability beyond Natural Language Processing (NLP) and cementing its position as a versatile and general-purpose neural network architecture.\\nIn this comprehensive guide, we will dissect the transformer model to its core, thoroughly exploring every key component from its attention mechanism to its encoder-decoder structure. Not stopping at the foundational level, we will traverse the landscape of large language models that leverage the power of the transformer, delving into their unique design attributes and functionalities. Further expanding the horizons, we will explore the applications of transformer models beyond NLP and probe into the current challenges and potential future directions of this influential architecture. Additionally, a curated list of open-source implementations and supplementary resources will be provided for those intrigued to explore further.\\nWithout bells and whistles, let’s dive in!\\n Figure 0: Transformer Architecture that we will explore in depth in this article. Adapted from (Vaswani et al. 2017).\\n\\n\\nNeural Networks Before Transformers\\nThe designers of transformer neural architecture were interested in finding an architecture that could work for sequence to sequence modelling. It wasn’t that there weren’t existing sequence modelling architectures, it’s just that they had many drawbacks. What are other kinds of neural networks that be used for sequence modelling? What are their drawbacks? Let’s seek the answers to those questions as we motivate transformers along the way.\\n\\nMultiLayer Perceptrons(MLPs)\\nLet’s start with multilayer perceptrons(MLPs), one of the classical neural network approaches. MLPs are not super powerful themselves but you will find them integrated in almost any other architecture(surprisingly even in transformer). MLPs are basically a sequence of linear layers or fully connected layers.\\n Figure 1: Multilayer Perceptrons(MLPs).\\nMLPs have long been used to model different kinds of data way before the AI community find best architectures for various modalities but one thing for sure, they are not suitable for sequence modelling. Due to their feedforward design, they can not preserve the order of information in a sequence. Sequence data lose meaning when the order of the data is lost. Thus, the inability of MLPs to preserve order of information make them unsuitable for sequence modelling. Also, MLPs takes lots of paramaters which is another undesired property a neural network can have.\\n\\n\\nConvolutional Neural networks\\nConvolutional neural networks(CNNs or ConvNets) are a class of neural network architectures that are most known for processing images and other modalities such as texts and videos.\\n Figure 2: Convolutional neural networks for text understanding(X. Zhang and LeCun 2015).\\nConvNets have so far been successful in small scale and large scale visual recognition but not quite successful in sequence modelling. They are easy to parallize(good for GPUs), due to their locality(computations are bundled in local parts of the input data), they require many layers to handle long-term dependencies. As opposed to images that have fixed length, most sequential data have variable length, something that neither ConvNets or MLPs can handle.\\n\\n\\nRecurrent Neural Networks\\nUnlike MLPs or ConvNets, recurrent neural networks(RNNs) were designed with sequence in mind. RNNs have feedback loop in their design, a key element in their ability to model sequential data. Another desirable property of RNNs is that they can handle variable length data.\\nThere are fundamental problems in how RNNs are wired. Firstly, due to their sequential design, they are likely to be unstable for long-term sequences. Secondly, they can not parallized which limit their scalability on modern machine learning accelerators(like GPUs).\\n\\nFigure 3: Recurrent neural networks(RNNs).\\nRecurrent networks have many variations. One of their famous version is Long Short Term Memories(LSTMs). LSTMs can handle long-term sequences. They have a cellstate(horizontal straight line in figure below) and gates which all smooth the flow of information.\\n\\nFigure 4: Long Short Term Memories(LSTMs).\\nAnother slightly efficient version of LSTMs is gate recurrent Units(GRUs). LSTMs works great for basic sequence modelling problems but they are still limited in how far they can go. As we previously said, they can not parallized which means they can not be scaled. Also, even if they can preserve the order of information, they can not reason about the global context of the data they are processing. Context is important. Take an example in machine translation(the task that basically gave us transformer), context of sentence being translated is as important as the order.\\nAll we have been doing basically is to motivate the transformers. So far, we have seen that prior neural networks were either not suitable for sequence modelling or not parallizable or not stable or limited in context length, all of which are primary desirable traits of sequence neural architectures.\\nNow that we have the right background, let’s dive into transformer architecture.\\n\\n\\n\\nTransformer Architecture\\nTransformer is a neural network architecture that can process sequential data such as texts, audios, videos, and images(as a sequence of image patches). Transformer does not use any recurrent or convolution layers. It’s fundamental layer is called Attention. It also contain other basic layers such as fully-connected layers, normalization layer(LayerNorm mostly)(Ba, Kiros, and Hinton 2016), embedding layer, and positional encoding layer. We will see what each of those layers performs in next sections.\\n\\nFigure 5: Transformer Architecture. Adapted from (Vaswani et al. 2017).\\nAs we alluded to in the beginning, transformer was initially introduced for machine translation, a task that demands processing two sequences(both input and output are sequences). Thus, the transformer model had two parts: encoder for processing the input and decoder for generating the output. More about encoder, decoder, and other layers are discussed below.\\n\\nEncoder\\nEncoder is one of the main blocks of the transformer architecture that is right at the input of input sequence. Encoder transforms input sequence into compressed representation. In the orginal transformer architecture, the encoder was repeated 6 times(this depends on overall size of architecture, it can be changed). Each encoder block has 3 main layers which are multi-head attention(MHA), layer norm, and MLPs(or feedforward according to the paper).\\nMulti-head attention and MLPs are referred to as sub-layers in the transformer paper. Between sublayers, there are layer normalization and dropout and residual connections in between(refer to diagram for correct flow of those layers).\\nThe number of encoder layers was 6 as said previously. The more the number of encoder layers, the larger the model, and the more the model is likely to capture the global context of the input sequences hence resulting in better task generalization.\\n\\n\\nDecoder\\nThe decoder is pretty much the same as encoder except additional multi-head attention that operated over the output of the encoder. The goal of the decoder is to fuse encoder output with the target sequence and to make predictions(or to predict the next token).\\nThe attention that takes the target sequence in decoder is masked to prevent the current token(being processed) from attending to subsquent tokens in the target sequence. If the decoder has access to a full target sequence, this would basically be cheating and can result in model that can not generalize beyond the training data.\\nDecoder is also typically repeated the same times as encoder. In the orginal transformer, the number of decoder blocks were also 6 blocks.\\n\\n\\nAttention\\n\\nWhat Really is Attention?\\nAttention is the principal element of transformer architecture. In essence, attention is a mechanism that can allow the neural network to pay more attention to the part of input data that contains meaningful information and pay less attention to the rest of the input.\\nThe attention mechanism was used in various tasks long before the introduction of transformer architecture. The idea of attention first appeared in neural machine translation(NMT) approach that used attention to find the set of positions in input sentence where the most relevant information is concentrated(Bahdanau, Cho, and Bengio 2014). Because their attention based NMT could align and translate jointly or simultaneously, it surprisingly performed well than previous approaches. As you can see in the image below, the network was able to find the correct order of words in a translated sentence, a feat that prior neural machine translation approaches struggled to achieve.\\n\\nFigure 6: Aligning the source sentence and target sentence in neural machine learning translation(Bahdanau, Cho, and Bengio 2014). The x-axis and y-axis shows the source sentence and translated sentence respectively. Each pixels indicates the attention weights of source(input) token with its corresponding target token. The diagonal attention represents words that are in corresponding order(ex: the agreement on the -> L’ accord sur la). Attention can figure out the correct word order(ex: European Economic Area -> zone économique européenne).\\nWhat’s going on in the image above? Can you spot something? The order of words was reversed in translated sentence wherever it make sense in target language. Thus, when translating a sentence, attention can give the model the ability to not only translate the sentence correctly, but to also translate it in the right order based on the context of the target language. In brief, attention can identify and preserve the context when translating one language to another.\\nAnother earlier work that used attention is found in neural image captioning(Xu et al. 2015). In this work, the authors used ConvNets for features extraction and RNNs with attention mechanism to generate a caption that aligns best with the input image. The image belows(taken from the paper) shows where the model roughly attends to.\\n\\nFigure 7: Generating caption with neural captioning model. The white regions show where the model is focusing when generating caption A woman is throwing a frisbee in a park”. Image from (Xu et al. 2015).\\nOn a global level, integrating attention mechanism in image captioning model helps the model to attend to the meaningful part of the input image when generating a caption.\\n\\nFigure 8: The model can attend to key objects when generating captions. Image taken from (Xu et al. 2015).\\nBoth the examples we used above demonstrate the effectiveness of attention. Attention is really a magic mechanism that allows the neural network to focus on part of input data that contains meaningful information and focus less on rest of the input data.\\nNow that we understand attention, let’s look at the inputs of attention function in transformer architecture: querry, keys, and values.\\n\\n\\nAttention Function: Query, Key, Value\\nIntuitively, attention is really “focus on most important part of the input data”. Technically speaking, attention measures the similarity between two vectors and return the weighted similarity scores. A standard attention function takes three main inputs which are query, key, and value vectors. Before breaking down the attention function, let’s try to understand what keys, values, and queries mean.\\nQuery, keys, and values are terms commonly used in search engines and database systems. To understand those terms, let’s take a simple example.1 Let’s say you are searching papers that are based on attention on ArXiv. The query is ideally what you will put in the search box. Internally, the ArXiv may organize papers by a set of predefined keys. Before ArXiv gives you papers that you asked for, it will compare your query to those predefined set of keys and return papers that best match with query and keys correspondence. Values merely refers to all papers in the database. As a disclaimer, we are using this example to understand the meaning of query, keys, and values in search and database systems context. It’s not an attempt to show how ArXiv system works.\\n\\nFigure 9: Example demonstrating query, keys, and values in ArXiv paper search system.\\nWith such intuitive understanding of query, keys, and values in mind, let’s move to the mathematical representation of the attention function.\\n\\\\[\\nAttention(Q, K, V) = Softmax(\\\\frac{QK^T}{\\\\sqrt{d_k}})V\\n\\\\]\\nFrom the function above, \\\\(Q\\\\), \\\\(K\\\\), \\\\(V\\\\) are query matrix, key matrix, value matrix respectively. We compute the dot product of query and keys and divide the product by a scaling factor of \\\\(\\\\sqrt{d_k}\\\\). The scaling factor is used to avoid the scenarios where large values of \\\\(QK^T\\\\) would result in small gradients. Then, we normalize the dot product into a probability distribution with softmax(this basically give us weighted sum) and by multiplying it with values, we get weighted values.\\n\\nFigure 10: Graphical representation of dot-product attention. Figure adapted from (Vaswani, 2017).\\nThe kind of attention described above is called scaled-dot product attention, a modified dot-product attention(Luong, Pham, and Manning 2015). There are other kinds of attention such as additive attention(Bahdanau, Cho, and Bengio 2014), content-based attention(Graves, Wayne, and Danihelka 2014), location-based attention(Bahdanau, Cho, and Bengio 2014), and general attention(Luong, Pham, and Manning 2015). Each of those attention types can either be applied globally(to the whole input data), hence global attention, or locally(sub-parts of the input data), hence local attention.\\nYou may have heard that transformer is parallizable and you may be wondering where it comes from. Transformer parallization comes from attention function. Provided that both query, keys, and values are matrices, attention can be performed in two main matrix multiplies and hence no loops or any recurrent operation involved. Computing attention is resonably faster for GPUs. For bigger models(in order of billions parameters) and massive training data(in order of billion/trillions tokens), attention is can be expensive since it takes quadratic time complexity from the fact that each token attends other tokens.\\n\\n\\n\\n\\n\\n\\nNote\\n\\n\\n\\nIf the queries, keys, and values are derived from same source, the attention applied to them is called self-attention. If they come from different source, we say cross-attention.\\n\\n\\n\\n\\nMulti-Head Attention\\nWhat we decribed above is a single attention layer. In practice, you typically would not get sound results with just one attention layer. Instead, people tend to compute multiple attention layers in parallel and concatenate the results. In nutshell, that is multi-head attention. Multi-head attention is basically multiple independent attentions computed over linearly projected QKV vectors. In the figure below of multi-head attention, the concatenated attention values are linearly projected to the model dimension.\\n\\nFigure 11: Multi-Head attention. Figure adapted from (Vaswani, 2017).\\nAs explained by the designers of the transformer architecture, computing multiple attentions in parallel allows the model to “jointly attend to information from different representation subspaces at different positions.”“(Vaswani et al. 2017). A surprising thing about multi-head attention is that it doesn’t increase the overall computation cost because the dimension of each head is oneth of number of heads(i.e, heads in base transformer is 8) of the overall model dimension(ie, 512). So, if the dimension of the model(\\\\(d_{model}\\\\) in the paper) is 512, the number of heads in multi-head attention are 8, each head is thus \\\\(512/8=64\\\\).\\nMulti-head attention can be seen as depth-wise separable convolution(Chollet 2017) in ConvNets. Depth-wise separable convolution is a special type of convolution that splits input tensor into multiple channels, operate on each channel independently, concatenate the individual outputs and and feed the results to a pointwise convolution(1x1 convolution which is equivalent to a linear projection).\\n\\n\\n\\nMLPs\\nMLPs or Multilayer Perceptrons2 are one of the two sublayers in both encoder and decoder. MLPs in the transformer are made of two linear layers with ReLU activation in between and they are applied to each position independently and identically.\\n\\nFigure 12:Multi-Layer Perceptrons(MLP) in transformer.\\n\\n\\nEmbeddings and Positional Encoding Layers\\nThe transformer architecture incorporates two embedding layers: one at the encoder to handle the input or source sequence, and another at the decoder for handling target or output sequence. These embedding layers convert input or output tokens into dense vectors of a fixed size, essentially mapping each token in a sequence to a specific dense vector. Utilizing embeddings is a standard practice in language modeling due to the semantic depth they provide. With these embedded token vectors, those bearing similar semantic meanings tend to align in the same direction.3.\\nThe size of the embeddings in the base transformer is 512(this is the dimension of the whole model). As a side note here, transformer architecture maintains the same dimension across the whole network and it is 512 for base model. This is what referred to as \\\\(d_{model}\\\\) previously.\\nPositional encodings serve as integral components in the initial stages of both the encoder and decoder within a Transformer model. They are used to preserve the order of tokens in a sequence. One might question the necessity of these positional embeddings. This stems from the inherent permutation invariance of the attention mechanism, whereby modifying the order of tokens does not alter the output weighted values4. Consequently, the attention mechanism, on its own, lacks awareness of the token order. As the transformer architecture does not incorporate any other recurrence methods, positional encodings are introduced to equip the model with positional awareness of the tokens in the sequence. In essence, without positional encodings, a Transformer would indeed exhibit permutation invariance. However, such a design would fall short for tasks where sequence order holds significance, as is the case for most NLP tasks.\\nFor encoding positional information in a sequence, the designers of transformer used sinusoidal functions of different frequencies. They also experimented with learned positional embeddings, but it did not make a difference in the results.\\n\\n\\nResidual Connections, Layer Normalization, and Dropout\\nResidual connections are at the heart of neural network design and they are one of the popular ingredients in modern deep learning. Since when deep residual networks proved substantial performance in computer vision(He et al. 2016), residual connections have been used in almost most neural networks not just in vision but in other modalities as well. In fact, it is almost impossible to see a neural network model that does not use residual connections in present times. Residual connections alleviate unstable gradient problems and they help the model to converge faster.\\nOne of the transformer authors, Ashish Vaswani once said that “residual connections carry positional information to higher layers, among other information.” Take a look at the image below!\\n\\nFigure 13: Residual connections carry signals to higher layers which improves the training of transformer model. The smooth diagonal in first image(with residuals) shows the effectiveness of residual connections. Image by Ashish Vaswani in CS224N.\\nLayer normalization(Ba, Kiros, and Hinton 2016) is also one of the most used normalization techniques in modern neural networks. Layer normalization significantly reduces the training time by normalizing the activations of a layer with the layer mean and variance. Unlike batch normalization(Ioffe and Szegedy 2015) that normalizes each layer with mean and variance computed over the mini-batch, layer norm just normalizes each layer with the mean and variance of each activation. Layer normalization maintains similar behavior during both training and testing phases, unlike batch normalization which exhibits different behaviors in these two stages.\\nThere are two ways to place layer normalization in transformer architecture. The first option is called Post layer normalization(Post-LN) where layer normalization is placed between residual blocks(or after each sublayer(multihead-attention and MLPs) but after addition). The second option is called Pre layer normalization(Pre-LN) where layer normalization is placed before each sublayer inside the residual block. The standard transformer architecture uses Post-LN, but in the updated codebase that trained the orginal transformer5, it was found that to be Pre-LN. This mismatch between paper and codes makes it hard to trace back the actual position of layer normalization in initial transformer but from the commit history, it looks like Pre-LN was used later. The authors could have updated the paper but they probably didn’t mind since no one knew this would turn out to be one of the influential and reference papers in neural network design.\\n\\nFigure 14: Post layer normalization(Post-LN) and Pre layer normalization(Pre-LN).\\nThus, it’s not exactly clear where the layer normalization should be and this is an active research question. A recent study on the impacts of Pre-LN and Post-LN(Xiong et al. 2020) showed that placing layer normalization before multi-head attention and MLPs(Pre-LN) improves the training and converge much faster than layer normization placed after multi-head attention and MLPs. The study also claimed that with Pre-LN, you don’t need to be smart at choosing learning-rate scheduler since Pre-LN have better initializations. Neither of Pre-LN an Post-LN is perfect. Another quite recent study introduced ResDual(Xie et al. 2023) which basically alleviates issues of Pre-LN and Post-LN by introducing additional residual connection with layer normalization.\\nWhere you should place layer normalization continue to be a question but this should be less of a question. As many people have noted, transformer seems to be a universal architecture. The orginal vanilla transformer(with few tweaks like yes LN) is the one that is still behind most novel works in language modelling, visual recognition, and multimodal learning depsite millions number of works that claims to improve the transformer. Thus, we should aim to keep the universality of this architecture. We will see this more in efficient transformers toward the end of the article.\\nBefore we wrap up this section, let’s talk about dropout(Srivastava et al. 2014) in the transformer architecture. Layer normalization can acts as a regularizer as a side effect but you still need other forms of network regularizations to deal with overfitting. Dropout is applied to the output of each sublayer(before addition and normalization). It is also applied to the sum of the embeddings and the positional encodings in both encoder and decoder stacks. For other regularization techniques used in training transformer and other training details, check out the paper for more.\\n\\n\\nLinear and Softmax Layers\\nThe linear layer after decoder takes the decoded activations and project them to the size of the vocabulary. This linear layer will basically produce logits. The softmax layer will take those logits and turn them into next-token probabilities. The next predicted token will be basically the argmax of softmax output.\\n\\n\\n\\nVisualizing Attention\\nAttention can capture the overall context from an input sequence, which often leads to better performance of the model. By visualizing attention, we can see which parts of the input sequence have significant influence on the model’s output. This helps us better understand how the inner workings of Transformer neural networks.\\n\\nFigure 15: Visualizing attention with ExBert.\\nThe figure above depicts the attention heads on 8\\\\(^{th}\\\\) layer of GPT-2(Radford et al. 2019). From the figure, it’s clear that even in the early layers of the transformer, most tokens attend to each other.\\nA number of tools that visualize attention have evolved overtime to help the deep learning community understand what’s going inside the transformer model. One of the most famous tools is BertViz(Vig 2019) 6. ExBert that we used to make the above visualization is also an excellent and simple tool for visualizing the attention on most transformer based models such as GPT-2 and BERT(Devlin et al. 2019).\\n\\n\\nThe Pros and Cons of Attention\\nThe attention mechanism has resulted in a significant shift in sequence modelling and other modalities that can be framed as sequences. When compared with other sequence networks such as recurrent networks and 1D convolutions, attention offers numerous advantages. These are briefly discussed below:\\n\\nLong-term Dependencies: Traditional Recurrent Neural Networks (RNNs), including variants like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), are prone to the issue of long-term dependencies, where the model’s ability to retain information weakens over time. Attention mechanisms help mitigate this problem by enabling the model to directly access any point in the input sequence, thereby preserving the overall context.\\nParallelization: Unlike RNNs, which require sequential computation, attention-based models, such as transformer architectures, can process all tokens in the input sequence in parallel. This makes them more computationally efficient and scales better with current hardware accelerators.\\nInterpretability: Attention provides a certain degree of interpretability, as it highlights the parts of the input that the model considers most important for producing a particular output. The “attention map” can help us understand what the model is “thinking.”\\nGlobal Context: In Convolutional Neural Networks (CNNs), the receptive field is typically local and depends on the kernel size, potentially leading to the loss of broader context. However, with attention, each output token can take into account information from every token in the input sequence, thus preserving the global context.\\nImproved Performance: Attention-based models, especially those that utilize transformer architectures, have achieved state-of-the-art performance in many NLP tasks, outperforming their RNN and CNN counterparts. They have also pushed envelope in other modalities such as computer vision, speech recognition, robotics, multimodal learning, etc…\\n\\nIn the figure below, we summarize the properties of attention-based models versus other deep neural network architectures.\\n\\nFigure 16: Attention versus other recurrent network architectures. Tranformer possesses nearly all good traits of neural networks. ConvNets are close to transformer but they require many layers to achieve long-range dependencies.\\nDespite the multitude of advantages they offer, as everything else in life, attention mechanisms also come with their fair share of challenges. For instance, in several types of attention, both memory consumption and computational cost can scale quadratically with sequence length. Various strategies, such as sparse attention or local attention, have been proposed to alleviate these issues but most of them are rarely used in practice(Tay et al. 2020).\\nWhile transformers offer the advantage of parallelization during training, the nature of the inference process may still necessitate a sequential approach, contingent on the specific task. Due to their autoregressive nature, transformers generate outputs one token at a time, continuing this iterative process until the desired output sequence is fully produced.\\nFurthermore, while attention offers a certain level of interpretability, it is far from perfect. Although it provides some insights into the model’s functioning, fully deciphering complex models based solely on attention maps can be, to say the least, a daunting task, if not almost impossible.\\n\\n\\nLarge Language Transformer Models\\n\\nEvolution of LLMs\\nLarge Language Models (LLMs) have revolutionized human interaction with machine learning systems. Natural language interfaces, such as ChatGPT and Bard, are powered by robust LLMs. These models have paved the way for executing natural language downstream tasks on-fly or through zero-shot learning. Such tasks, in the past, necessitated the gathering of a downstream or task-specific datasets.\\nAt the core of these LLMs, it’s fundamentaly a transformer model that we have seen with little tweaks here and there. In this section, we will delve into the compressed evolution of Large Language Models. Moreover, we will explore the development of vertical LLMs, specifically designed and fine-tuned for particular applications.\\nTransformer base model had 65M parameters but since then, language models got bigger and bigger(in order of billions) and hence the name large language models. Below is a quick overview of popular large language models.\\n\\nFigure 17: Overview of popular LLMs. Layers are number of stacked encoders/decoders or both for encoder-decoder models, width is the dimension of the model, heads are number of attention layers in multi-head attention, params are number of parameters. N.B, the numbers of heads in GPT-2 are not exactly known.\\nThe training process for most large language models (LLMs) follows a broadly similar pattern. In the initial pretraining phase, LLMs are exposed to vast volumes of curated textual data, sourced from a diverse range of materials such as books, articles, code snippets, and websites. This vast dataset is essential for the models to gain a comprehensive understanding of the world, enabling them to create rich representations and generate contextually relevant responses. The general public holds high expectations for LLMs’ performance across various domains. To meet these expectations, the pretraining data must encompass a wide spectrum of topics and disciplines(J. Yang et al. 2023).\\nThe actual training of LLMs occurs in an unsupervised fashion, with a specific focus on self-supervised learning(SSL). This approach eliminates the need for labelled data, a crucial feature considering the near-impossibility of labeling the entirety of online content.\\n\\nFigure 18: A typical training workflow of large language models. LLMs are typically trained on large unlabelled dataset. After, they can be used directly via prompt engineering or they can be fine-tuned further on specialized tasks.\\nHowever, training models on unlabelled data requires the clever implementation of training objectives since there is no ground truth for reference. Most LLMs, therefore, utilize the next-token prediction (NTP) as a common training objective. In essence, the LLMs are taught to accurately predict the next token in a sequence, gradually enhancing their understanding and generating capabilities. Another commonly used training objective is masked language modelling(MLM). Masked language models are trained to predict a masked token in a sequence. This objective was popularized by BERT(Devlin et al. 2019).\\nAfter pretraining phase, the models can be used to generate texts via techniques like zero-shot learning or few-shots learning. In zero-shot learning, a model is prompted to perform a task(or answer a given question) without any demontrations of how the task is done. In few-shots learning, a model is given a number of demonstrations of how the task is done before it can be asked to perform that task. Zero-shot learning and few-shot learning are examples of in-context learning. In-context learning(ICL) refers to the ability of LLMs to generate coherent texts using semantic prior knowledge(Jerry Wei et al. 2023) and without any parameter updates(Akyürek et al. 2023). Prompting large language models(also known as prompt engineering) is a relatively new field itself and there are other prompt engineering techniques such as chain of thoughts(CoT)(Jason Wei, Nye, and Liang 2022).\\nIn-context learning tends to excel at tasks that are considered simple but falls short for tasks that can not be described easily in prompts. Complex tasks requires more than clever prompts. In the words of Karpathy, “reaching top tier performance(on complex tasks) will include finetuning, especially in applications with concrete well-defined tasks where it is possible to collect a lot of data and”practice” on it.”7. Thus, for LLMs to get good performance on specialized tasks like mathematics, medicine, scientific fields(like chemistry), people typically finetune base LLMs on downstream datasets. We will see examples of this in the section of vertical LLMs.\\nNow that we’ve briefly introduced Large Language Models (LLMs), it’s time to examine some of the most popular LLMs, focusing specifically on their design choices: whether they function as encoders, decoders, or employ a combined encoder-decoder architecture.\\n\\n\\nEncoder, Decoder, Encoder-decoder LLMs\\nThe standard transformer model has encoder-decoder and this has to do with the task it was meant to perform which is machine translation where you have to process both input sentence and its target translation. Since the transformer, AI research community came up with different variations of the architecture for different tasks. Depending on the task, some transformer models maintained encoder-decoder structure, some used decoder only or encoder only. Let’s start with the latter.\\n\\nEncoder-only LLMs\\nEncoder-only LLMs use the encoder part of the standard transformer model. Encoder-only LLMs are typically used for NLP discriminative tasks such as text classification and sentiment analysis.\\nBERT(Devlin et al. 2019) is one of most popular encoder-only language models. BERT is one of the earliest works that showed that you can pretrain a transformer(encoder) on large unlabeled text dataset and finetune the same architecture on various downstream tasks with additional task-specific head. The pretraining objectives for BERT were masked language modelling(MLM) and next sentence prediction(NSP)8. With masked language modeling, we mask a given percentage(15% as noted in the paper) of input tokens and the goal is to predict the masked tokens. In next sentence prediction, for two sentences pair making up the input sequence, the goal is to predict whether or not two sentences are in a correct order at random.\\n\\nFigure 19: Masked language modelling(MLM) in BERT. In the sentence example shown in the figure, the objective of training BERT is to predict the masked word “network”. In next sentence prediction objective, the workflow is roughly the same but instead of predicting the masked tokens, we predict if two sentence pairs separated by SEP token are in correct order.\\nBERT is a truly revolutionary technique that improved SOTA on ubiquitous number of NLP downstream tasks. It also inspired other efficient bidirectional architectures for NLP pretraining such as RoBERTa(Y. Liu et al. 2019) standing for Robustly optimized BERT approach. One of the main design choices that RoBERTa introduces is not using next sentence prediction objective.\\n\\n\\nDecoder-only LLMs\\nDecoder-only LLMs are based on the decoder part of standard transformer. In transformer architecture, decoder is highly similar to encoder except that the self-attention in decoder is masked to prevent the model to look at subsequent tokens when generating current token.\\nDecoder LLMs are trained with next token prediction objective9. As a result, they can only generate one token at time or autoregressively. Overally, decoder models are used in generative tasks.\\nThe most popular decoder models are GPT(Generative Pretrained Transformer) models family, most notably GPT-3(Brown et al. 2020) and GPT-4(OpenAI 2023). GPT-3 and GPT-4 are direct scale-up of the early GPT model(Radford et al. 2018). As any other large language model, GPT models are trained on massive amount of unlabelled data(in order of billions to trillions tokens). Due to the large-scale pretraining and suitable training objective, GPT models develops impressive in-context learning capabilities where they can perform a range of NLP downstream tasks without gradient updates or task-specific fine-tuning(Brown et al. 2020). In fact, GPT models can perform tasks like text classification, summarization, question answering on-fly by just prompting the model in zero-shot or few-shot settings10. This remarkable feat of in-context learning has often been called “emergent abilities” of large language models(Jason Wei et al. 2022).\\nGPT models are not the only models based on decoder. In fact, most famous LLMs are decoders. Examples include PaLM(Chowdhery et al. 2022), BLOOM(Le Scao et al. 2022), Chinchilla(Hoffmann et al. 2022), LLaMA(Touvron et al. 2023), and many others.\\n\\n\\nEncoder-Decoder LLMs\\nEncoder-decoder LLMs looks like the standard transformer. They are generally used in tasks that demands processing two sequences(i.e, input and target are both sequences) such as machine translation. Encoder-decoder style is not widely used compared to other model styles we have seen. The most famous models of this kind are T5(Raffel et al. 2019), BART(Lewis et al. 2019), UL2(Tay et al. 2022), FlanT5(Chung et al. 2022), mT5(Xue et al. 2021), etc…\\nEncoder-decoder style is also used in multimodal learning, most notably vision-language pretraining(VLP). Works like SimVLM(Z. Wang et al. 2021) and PaLI-X(X. Chen et al. 2023) employs encoder for learning joint image and text representations and decoder for generating the output.\\n\\n\\n\\nVertical LLMs\\nMost of LLMs that we outlined above are typically referred to as foundational or frontier LLMs. Foundational models are typically trained on massive amount of data with self-supervision and they can be fine-tuned to a wide range of downstream tasks(Bommasani et al. 2022).\\nVertical LLMs are a class of LLMs that are adapted to specific applications. Foundational LLMs can generalize to simple tasks like sentiment analysis but they don’t perform well on complex tasks or tasks that require a domain expertize. For example, a foundational LLM is unlikely to perform well on medical question answering task because it doesn’t have expertize in medicine. More examples: a foundational LLM is unlikely to perform well on legal question answering task because it doesn’t have expertize in law. This is also true in other fields such as finance, physics, chemistry, etc…Vertical LLMs are designed to address this issue. They are trained on a specific domain and they can perform well on tasks that require expertize in that domain. Foundational models aim to be generalists but most of the time, we care about models that can do one thing very well.\\nExamples of recent vertical LLMs include MedPaLM(Singhal et al. 2022) and Med-PaLM 2, ClinicalGPT(G. Wang et al. 2023), FinGPT(H. Yang, Liu, and Wang 2023), BloombergGPT(Wu et al. 2023), Galactica(Taylor et al. 2022), Minerva(Lewkowycz et al. 2022), among others.\\n\\nFigure 20: LLMs Topologies. Adapted from (J. Yang et al. 2023).\\n\\n\\n\\nTransformers Beyond NLP: Vision and other Modalities\\nTransformer was introduced for Natural Language Processing(NLP) domain, more precisely, for neural machine translation. In no time, transformers outperformed prior neural networks on most NLP tasks and quickly expanded into other modalities. In this section, we will discuss in brief the emergence of transformers in visual recognition and other modalities.\\nVisual recognition is one of the earliest modalities that was significantly impacted by transformers. For a long time, ConvNets were state of the arts in visual recognition. It’s thus a critical to ask why researchers care about alternatives to ConvNets. The main downside of ConvNets is their spatial inductive biases11.\\nOne of the earliest applications of transformer to image processing is Image Transformer (Parmar et al. 2018) which approached image generation as an autoregressive problem, analogous to text generation. The Image Transformer was a standard transformer applied to a sequence of pixels, trained to generate these pixels autoregressively until it created the complete image. This was a great idea, but as it turns out, images typically have large resolutions, and thus, it was not feasible to apply self-attention to images of 256x256 for instance. There were several works attempting to apply transformer to image domain but one of the first successful works was Vision Transformer(Dosovitskiy et al. 2021) that applied the transformer encoder to a sequence of images patches. ViT overcame the computational complexities of self-attention by image patchification idea, marking a significant step in extending transfomers to computer vision domain.\\nAs we saw early, a huge contribution of transformers successes in NLP was unsupervised pretraining on massive amount of unlabelled data. The success of Vision Transfomer was also attributed to millions of training images, JFT-300M(C. Sun et al. 2017) although later works like MAE(He et al. 2021) and (Steiner et al. 2021) achieved resonably good performance on classical computer vision benchmarks such as ImageNet. MAE is an encoder-decoder self-supervised model that follows BERT pretraining objective of predicting randomly masked patches while the later explores clever augmentations and regularizations to train ViT. ViT has been used as backbone in many influential papers such as CLIP(Radford et al. 2021), DALLE•2(Ramesh et al. 2022), Stable Diffusion(Rombach et al. 2022), among other recent works in visual language models. Aside from ViT enabling joint modelling of vision and language, it has also been augmented with convolutional neural networks to get both worlds in computer vision downstream tasks. Notable works of ConvNets and Vision Transformer topology are DETR(Carion et al. 2020), PatchConvNet(Touvron et al. 2021), MobileViT(Mehta and Rastegari 2022), among others.\\nVision and language are two of the most important modalities when it comes to human to computer interaction and it’s not surprising that most works incorporating transformers have been in language, vision, or visual language learning. That said, transformers have been used in other modalities such as reinforcement learning(L. Chen et al. 2021), robotics((Brohan et al. 2022), RoboCat(Bousmalis et al. 2023)), and speech recognition(Radford et al. 2022). Finally, works such as Gato(Reed et al. 2022) and ImageBind(Girdhar et al. 2023) have gone further in modelling pretty much all modalities.\\nTransformer has established itself as universal architecture and recent works across different modalities prove that, but there are still challenges.\\n\\n\\nTransformer: Current Challenges and Future Directions\\n\\nEfficient Transformers\\nTransformer has shown significant performance across various modalities such as language, vision, robotics, and reinforcement learning. Transformer neural network architecture has a set of traits that make it a suitable architecture for those domains: it is expressive, plays well with current optimization techniques, and it can be parallized. From those traits, one can say that transformer is an efficient architecture. That said however, the efficiency of transformer comes with enormous computatation cost due to the quadratic time and memory complexity of self-attention. The compute requirements of transformer has limited its scalability and its applications in low-budget devices such as smartphones and microcontrollers.\\nModel efficiency is an important thing to take into account when developing and deploying machine learning systems because how a model perform during inference can affects user experience(Dehghani et al. 2022). There has been zillion transformer models that claim to improve the efficiency(memory footprint and computational cost) of transformer architecture(those models are typically called “xformers”) but those models usually tend to be targeted at one particular benchmark or device. Most of the new xformers models that claim to reduce the quadratic time and memory complexity of self-attention are much slower than vanilla transformer and they are rarely used in practice and they don’t have the universality of original transformer(Tay et al. 2020).\\nAs (Tay et al. 2020) puts it nicely in a survey of “Efficient Transformers”, the ideal xformer should yes reduce the quadratic time complexity of self-attention, but should stay universal and perform well across all tasks and modalities. It should also not trade-off speed for memory, should not be hard-engineered, should stay elegant and simple. For more, I recommend you read the survey paper of efficient transformers.\\n Figure 21: A taxonomy of efficient transformers. Image from (Tay et al. 2020) .\\nVirtually all modified transformer models compute the approximation of attention to reduce the cost down. As opposed to those approaches, there is actually one kind of attention that computes exact standard attention values but way faster. That approach is FlashAttention(Dao et al. 2022) and we will talk about it on a high-level.\\nFlashAttention is fast and memory-efficient algorithm that computes the exact attention. FlashAttention is 2-4x faster than standard attention. It achieves this enormous increase in compute efficiency by using two main techniques: tiling and recomputation. Tiling happens in forward pass and it involves splitting large matrices in attention(K key and V value) into blocks. Rather than computing attention over entire matrices, FlashAttention computes it over blocks and concatenate the resulting blocks saving a huge amount of memory. Recomputation happens in backward pass and it basically means recomputing the attention matrix rather than storing it in forward. The idea of FlashAttention boils down to improving the memory and not decreasing computations because modern GPUs have high theorical FLOPs(Floaping Point Operations, means you want to max that out) but limited memory12(means any saving in memory can improve the training speed). HBM(High Bandwidth Memory) is typically large but it is not faster than on-chip SRAM(Static Random Access Memory) and thus, the computations over blocks(of K and V) happens in SRAM(because it is faster) but all full matrices are stored in HBM(because it’s big). This high-level explanation is probably oversimplication provided that FlashAttention is implemented at the GPU level(with CUDA software) and this is in fact the reason why it is IO aware but hopefully that explains what’s going on in this fast algorithm.\\nBelow image shows the memory hierarchy in GPU, FlashAttention algorithm, and amount of time(in ms) taken by each intermediate step in GPT-2 attention versus FlashAttention. Ideally, we would want the bulk of computations to be taken by matrix multiplication(matmul) operations but surprisingly, dropout, softmax, and mask(i.e, GPT-2 is decoder model) end up taking the whole runtime in GPT-2 attention because they are computed over full matrices. Matmuls take less runtime than those other operations because GPUs are exactly designed to be fast at matrix multiplications(they have really high theorical FLOPs and maximizing FLOPs usage doesn’t reduce the runtime). By using tiling and recomputation techniques, the compute time of FlashAttention is significantly low compared to standard attention as you can see below.\\n\\nFigure 22: The memory hierachy in GPU, FlashAttention algorithm, and runtime of GPT-2 attention vs FlashAttention.\\nFlashAttention is intergrated in PyTorch 2.0, Hugging Face transformers, Microsoft’s DeepSpeed, MosaicML composer library and many other library. You can learn more FlashAttention in the paper, or watch this video by core author, and the release blogpost. At the time of writing this section, FlashAttention2(Dao 2023) was also released and it is even faster than FlashAttention version 1 on several orders of magnitude. FlashAttention-2 improves parallelism by parallelizing over sequence length dimension instead of batch size and number of attention heads and splits Q(query) matrix instead of K and V. This release blog post explains well what FlashAttention2 brings to the tensor table.\\n\\n\\nTransformers with Effective Long Contexts\\nHandling long context length is one of the main active areas of research in Transformer large models. As direct consequence of the quadratic time and memory complexity of attention, transformer fails to process long context windows. Researching techniques that extend the context window of transformer architecture is an important thing since context window determines the amount of information that you can fit in transformer memory during inference. Tasks like long conversations, summarizing long documents, and executing long-term planning may require models that support long context windows(S. Chen et al. 2023).\\nAlot have been written about context windows and extending them such as (S. Sun et al. 2021), but I want to highlight a recent paper that presents remarkable findings around long contexts. Recent language models(based on transformer) can take longer contexts but it’s not clear whether long context actually helps. As shown by (N. F. Liu et al. 2023), the performance of language models degrades with increase in input context length. So, even for models that have extended context length, their performance still degrades for longer input contexts. Also, the work also found that language models perform well when the relevant information are placed at the beginning or the end of the input context and significantly degrades when the relevant information are placed in the middle, suggesting that language models are U-shaped reasoners.\\nThe findings highlighted above are appealing and provide broad implications that could be applicable in the design of fine-tuning datasets and during in-context learning, but it’s important to note that none of those is established understandings provided that “how transformer models perform on long context windows” is an active area of research. We hope that future transformer models will be able to operate over long input sequences and at the same time perform well regardless of relevant information are placed. This is in fact the holy grail of large language models.\\n Figure 23: Language models(based on transformer) tends to perform well when relevant information are at the beginning or at the end of input context(graph on the left) and their performance decreases for longer contexts(graph on the right). The graphs are taken from (N. F. Liu et al. 2023).\\n\\n\\nMultimodal Transformer\\nA primary objective in neural network design is to architect a single, universal model that can efficiently process multiple modalities without necessitating modality-specific encoders or preprocessing. Indeed, transformer models have seen widespread application across various domains, spanning text, images, robotics, and speech. Yet, the goal of creating a truly universal transformer — one that performs equally effectively across all modalities without requiring specific adjustments — remains a challenge. This challenge arises from the inherent differences and complexities in data types and the transformer model itself, which frequently demand modality-specific modifications.\\nFor instance, the process for handling text, images, and speech each have unique considerations due to their individual characteristics. Transformers excel in scenarios where data can be framed as a sequence of tokens, however, the method of transposing a particular modality into such a sequence significantly varies among different modalities. Consequently, the challenge lies in designing a singular architecture that can uniformly extract valuable insights from all data types with comparable efficiency.\\nThe achievement of such an architecture would signify a monumental stride in the field of multimodal learning, paving the way for models that can seamlessly transition between different types of data and potentially unlocking new avenues of exploration in multimodal representation learning.\\nNearly all current state-of-the-arts in multimodal learning typically uses separate tokenizer and encoder for each modality and most of them are also designed for visual language learning. This section doesn’t dive deep into the specifics of current multimodal approaches based on transformers but we provide examples for people interested in diving deep: Flamingo(visual language)(Alayrac et al. 2022), Gato(Reed et al. 2022), ImageBind(Girdhar et al. 2023), OFA(P. Wang et al. 2022), Unified-IO(Lu et al. 2022), Meta-Transformer(Y. Zhang et al. 2023), among others.\\n\\n\\n\\n\\n\\n\\nNote\\n\\n\\n\\nVirtually all transformer challenges stem from its extreme compute and memory requirements. Truly efficient transformers such as FlashAttention could potentially alleviate those challenges.\\n\\n\\n\\n\\n\\nOpen-source Implementations of Transformer\\nThe original transformer model was implemented in Tensor2Tensor library13 but this was deprecated recently. The successor of of Tensor2Tensor is Trax which is based on JAX14.\\nThere are many open-source implementations of transformer model architecture. Let’s briefly talk about three of most popular implementations. HuggingFace Transformer library(Wolf et al. 2020) is arguably one of the most popular implementations of transformers. The library simplifies inference pipelines for NLP(and vision) downstream tasks and can be used to train or finetune transformer-based models. HuggingFace Transformer library is easy to use, it’s clean, and has a large community of developers and contributors. minGPT and nanoGPT by Andrej Karpathy are also popular implementations in open-source and research community. Furthermore, x-transformers provides concise and experimental implementations of various transformer models usually from new research papers.\\nLastly, it’s unlikely you will need to implement transformer model or part of it from scratch because modern deep learning frameworks such as PyTorch, Keras, and JAX(Via Flax) provides its implementation as layers that you can import easily just like how you import convolution or linear layers.\\n\\n\\nSupplementary Resources\\nThis article contributes to an existing pool of knowledge surrounding the understanding of transformer neural network architecture. Therefore, it would be remiss not to highlight some invaluable resources on transformer architecture, which we will briefly provides below:\\n\\nThe Annotated Transformer: This is one of the best and practical resources. It provides line-by-line implementation of transformer architecture with completely usable code. The original version was written by Sasha Rush and recent version was written by Austin Huang and his colleagues.\\nLet’s Build GPT from Scratch by Andrej Karpathy: This is arguably the best resource regarding implementations of transformer, most notably, GPT(Generative Pre-training Transformer). Karpathy builds and trains entire GPT from scratch, providing a decent explanation of every step along the way. Here is a lecture video and accompanying code repository(nanoGPT).\\nStanford CS25: Transformers United V2 aims at examining how transformers work and how they are applied in different fields from NLP, CV, biology to robotics and more. This course contains excellent talks from researchers. The introductory class of recent version of the course delves into transformer architecture and it is given by Karpathy, someone who deeply understands the intricacies of neural networks.\\nFormal Algorithms for Transformers provides a mathematical overview and formal algorithms of various transformer architectures.\\nTransformer Taxonomy provides an excellent literature review of transformer models, architectural changes since the inception of standard transformer, post pre-training techniques and 3 training techniques.\\nThe Illustrated Transformer is a remarkable blog post that break the transformer model apart and explains each part intuitively.\\nTransformer and attention blog series by Lilian Weng also provide excellent understanding of transformer and attention mechanism. A notable example of relevant Lilian Weng blogs are The Transformer Family Version(there is also version 2 of this blog) and Attention? Attention!.\\nAttention is All You Need Video by Yannic Kilcher walkthroughs the paper, explaining all the relevant concepts and related works well.\\nTransformer models: an introduction and catalog is also another resource that is worth mentioning. It provides a decent catalog of popular transformer models.\\n\\n\\n\\nConclusion\\nThe significance of transformer neural network architecture can not be overstated in the field of deep learning and computer science. The transformer model, initially introduced for neural machine translation has evolved into a versatile and general-purpose architecture, demonstrating impressive performance beyond natural language processing into other various modalities.\\nThroughout this article, we have delved into the core mechanics of the transformer and its essential components - its encoder and decoder structure, attention mechanism, multi-head attention, MLPs, embedding, positional encoding layers, and more. We have explored several benefits of self-attention, along with potential drawbacks. Also, by examining the visualization of attention, we have gained a deeper understanding of how transformers focus on different parts of the input sequence to generate outputs.\\nTransformers are at the core of large language models(LLMs) which has taken the world by a storm recently. We have seen evolution of LLMs and their different design styles, and the applications of transformers beyond NLP. We have also talked their current challenges, including the need for more efficient models and the effective use of context window. These challenges present exciting opportunities for future research and improvements.\\nAs deep learning field continues to evolve, transformer architecture remains a foundational building block of modern machine learning systems. There are many variations of transformer architectures, but regardless of what the future of transformers holds, one thing has been certain - attention is all you need. Stay curious, keep learning, and always pay attention!\\n\\n\\n\\nReferences\\n\\nAkyürek, Ekin, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2023. “What Learning Algorithm Is in-Context Learning? Investigations with Linear Models.” arXiv Preprint arXiv:2211.15661.\\n\\n\\nAlayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, et al. 2022. “Flamingo: A Visual Language Model for Few-Shot Learning.” arXiv Preprint arXiv:2204.14198.\\n\\n\\nBa, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. “Layer Normalization.” arXiv Preprint arXiv:1607.06450.\\n\\n\\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” arXiv Preprint arXiv:1409.0473.\\n\\n\\nBommasani, Rishi, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, et al. 2022. “On the Opportunities and Risks of Foundation Models.” arXiv Preprint arXiv:2108.07258.\\n\\n\\nBousmalis, Konstantinos, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X Lee, Maria Bauza, Todor Davchev, et al. 2023. “RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation.” arXiv Preprint arXiv:2306.11706.\\n\\n\\nBrohan, Anthony, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, et al. 2022. “RT-1: Robotics Transformer for Real-World Control at Scale.” arXiv Preprint arXiv:2212.06817.\\n\\n\\nBrown, Tom B, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” arXiv Preprint arXiv:2005.14165.\\n\\n\\nCarion, Nicolas, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. “End-to-End Object Detection with Transformers.” arXiv Preprint arXiv:2005.12872.\\n\\n\\nChen, Lili, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. “Decision Transformer: Reinforcement Learning via Sequence Modeling.” arXiv Preprint arXiv:2106.01345.\\n\\n\\nChen, Shouyuan, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. “Extending Context Window of Large Language Models via Positional Interpolation.” arXiv Preprint arXiv:2306.15595.\\n\\n\\nChen, Xi, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, et al. 2023. “PaLI-x: On Scaling up a Multilingual Vision and Language Model.” arXiv Preprint arXiv:2305.18565.\\n\\n\\nChollet, François. 2017. “Xception: Deep Learning with Depthwise Separable Convolutions.” arXiv Preprint arXiv:1610.02357.\\n\\n\\nChowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Bosma Maarten, Mishra Gaurav, Roberts Adam, Barham Paul, et al. 2022. “PaLM: Scaling Language Modeling with Pathways.” arXiv Preprint arXiv:2204.02311.\\n\\n\\nChung, Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, et al. 2022. “Scaling Instruction-Finetuned Language Models.” arXiv Preprint arXiv:2210.11416.\\n\\n\\nDao, Tri. 2023. “FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning.” arXiv Preprint arXiv:2307.08691.\\n\\n\\nDao, Tri, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.” arXiv Preprint arXiv:2205.14135.\\n\\n\\nDehghani, Mostafa, Anurag Arnab, Lucas Beyer, Ashish Vaswani, and Yi Tay. 2022. “The Efficiency Misnomer.” arXiv Preprint arXiv:2110.12894.\\n\\n\\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–86.\\n\\n\\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” In International Conference on Learning Representations.\\n\\n\\nGirdhar, Rohit, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023. “ImageBind: One Embedding Space to Bind Them All.” arXiv Preprint arXiv:2305.05665.\\n\\n\\nGraves, Alex, Greg Wayne, and Ivo Danihelka. 2014. “Neural Turing Machines.” arXiv Preprint arXiv:1410.5401.\\n\\n\\nHe, Kaiming, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2021. “Masked Autoencoders Are Scalable Vision Learners.” arXiv Preprint arXiv:2111.06377.\\n\\n\\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770–78.\\n\\n\\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv Preprint arXiv:2203.15556.\\n\\n\\nIoffe, Sergey, and Christian Szegedy. 2015. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” In International Conference on Machine Learning, 448–56.\\n\\n\\nLe Scao, Teven, Angela Fan, Christopher Akiki, Pavlick Ellie, Ilić Suzana, Hesslow Daniel, Castagné Roman, et al. 2022. “BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.” arXiv Preprint arXiv:2211.05100.\\n\\n\\nLewis, Mike, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019. “BART: Denoising Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension.” arXiv Preprint arXiv:1910.13461.\\n\\n\\nLewkowycz, Aitor, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, et al. 2022. “Solving Quantitative Reasoning Problems with Language Models.” arXiv Preprint arXiv:2206.14858.\\n\\n\\nLiu, Nelson F., Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. “Lost in the Middle: How Language Models Use Long Contexts.” arXiv Preprint arXiv:2307.03172.\\n\\n\\nLiu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” arXiv Preprint arXiv:1907.11692.\\n\\n\\nLu, Jiasen, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. 2022. “Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks.” arXiv Preprint arXiv:2206.08916.\\n\\n\\nLuong, Minh-Thang, Hieu Pham, and Christopher D Manning. 2015. “Effective Approaches to Attention-Based Neural Machine Translation.” arXiv Preprint arXiv:1508.04025.\\n\\n\\nMehta, Sachin, and Mohammad Rastegari. 2022. “MobileViT: Light-Weight, General-Purpose, and Mobile-Friendly Vision Transformer.” arXiv Preprint arXiv:2110.02178.\\n\\n\\nOpenAI. 2023. “GPT-4 Technical Report.” arXiv Preprint arXiv:2303.08774.\\n\\n\\nParmar, Niki, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. 2018. “Image Transformer.” In Proceedings of the 35th International Conference on Machine Learning, 4055–64.\\n\\n\\nRadford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models from Natural Language Supervision.” In International Conference on Machine Learning, 8748–63.\\n\\n\\nRadford, Alec, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. “Robust Speech Recognition via Large-Scale Weak Supervision.” arXiv Preprint arXiv:2212.04356.\\n\\n\\nRadford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. “Improving Language Understanding by Generative Pre-Training.”\\n\\n\\nRadford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.” OpenAI Blog 1 (8).\\n\\n\\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” arXiv Preprint arXiv:1910.10683.\\n\\n\\nRamesh, Aditya, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. “Hierarchical Text-Conditional Image Generation with CLIP Latents.” arXiv Preprint arXiv:2204.06125.\\n\\n\\nReed, Scott, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, et al. 2022. “A Generalist Agent.” arXiv Preprint arXiv:2205.06175.\\n\\n\\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. “High-Resolution Image Synthesis with Latent Diffusion Models.” In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10684–95.\\n\\n\\nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, et al. 2022. “Large Language Models Encode Clinical Knowledge.” arXiv Preprint arXiv:2212.13138.\\n\\n\\nSrivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” Journal of Machine Learning Research 15 (56): 1929–58.\\n\\n\\nSteiner, Andreas, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. 2021. “How to Train Your ViT? Data, Augmentation, and Regularization in Vision Transformers.” arXiv Preprint arXiv:2106.10270.\\n\\n\\nSun, Chen, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. 2017. “Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.” In Proceedings of the IEEE International Conference on Computer Vision, 843–52.\\n\\n\\nSun, Simeng, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. 2021. “Do Long-Range Language Models Actually Use Long-Range Context?” In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), 807–22. Online; Punta Cana, Dominican Republic: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.emnlp-main.62.\\n\\n\\nTay, Yi, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. “Efficient Transformers: A Survey.” arXiv Preprint arXiv:2009.06732.\\n\\n\\nTay, Yi, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, et al. 2022. “UL2: Unifying Language Learning Paradigms.” arXiv Preprint arXiv:2205.05131.\\n\\n\\nTaylor, Ross, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. “Galactica: A Large Language Model for Science.” arXiv Preprint arXiv:2211.09085.\\n\\n\\nTouvron, Hugo, Matthieu Cord, Alaaeldin El-Nouby, Piotr Bojanowski, Armand Joulin, Gabriel Synnaeve, and Hervé Jégou. 2021. “Augmenting Convolutional Networks with Attention-Based Aggregation.” arXiv Preprint arXiv:2112.13692.\\n\\n\\nTouvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, et al. 2023. “LLaMA: Open and Efficient Foundation Language Models.” arXiv Preprint arXiv:2302.13971.\\n\\n\\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” arXiv Preprint arXiv:1706.03762.\\n\\n\\nVig, Jesse. 2019. “A Multiscale Visualization of Attention in the Transformer Model.” In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 37–42.\\n\\n\\nWang, Guangyu, Guoxing Yang, Zongxin Du, Longjun Fan, and Xiaohu Li. 2023. “ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation.” arXiv Preprint arXiv:2306.09968.\\n\\n\\nWang, Peng, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. “OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework.” In Proceedings of the 39th International Conference on Machine Learning, 23318–40. PMLR.\\n\\n\\nWang, Zirui, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. 2021. “SimVLM: Simple Visual Language Model Pretraining with Weak Supervision.” arXiv Preprint arXiv:2108.10904.\\n\\n\\nWei, Jason, Max Nye, and Percy Liang. 2022. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv Preprint arXiv:2201.11903.\\n\\n\\nWei, Jason, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, et al. 2022. “Emergent Abilities of Large Language Models.” arXiv Preprint arXiv:2206.07682.\\n\\n\\nWei, Jerry, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, et al. 2023. “Larger Language Models Do in-Context Learning Differently.” arXiv Preprint arXiv:2303.03846.\\n\\n\\nWolf, Thomas, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, et al. 2020. “Transformers: State-of-the-Art Natural Language Processing.” In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 38–45. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.emnlp-demos.6.\\n\\n\\nWu, Shijie, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. “BloombergGPT: A Large Language Model for Finance.” arXiv Preprint arXiv:2303.17564.\\n\\n\\nXie, Shufang, Huishuai Zhang, Junliang Guo, Xu Tan, Jiang Bian, Hany Hassan Awadalla, Arul Menezes, Tao Qin, and Rui Yan. 2023. “ResiDual: Transformer with Dual Residual Connections.” arXiv Preprint arXiv:2304.14802.\\n\\n\\nXiong, Ruibin, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020. “On Layer Normalization in the Transformer Architecture.” In International Conference on Machine Learning, 10524–33.\\n\\n\\nXu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S Zemel, and Yoshua Bengio. 2015. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” In International Conference on Machine Learning, 2048–57.\\n\\n\\nXue, Linting, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. “mT5: A Massively Multilingual Pre-Trained Text-to-Text Transformer.” In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 483–98.\\n\\n\\nYang, Hongyang, Xiao-Yang Liu, and Christina Dan Wang. 2023. “FinGPT: Open-Source Financial Large Language Models.” arXiv Preprint arXiv:2306.06031.\\n\\n\\nYang, Jingfeng, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. “Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.” arXiv Preprint arXiv:2304.13712.\\n\\n\\nZhang, Xiang, and Yann LeCun. 2015. “Text Understanding from Scratch.” arXiv Preprint arXiv:1502.01710.\\n\\n\\nZhang, Yiyuan, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, and Xiangyu Yue. 2023. “Meta-Transformer: A Unified Framework for Multimodal Learning.” arXiv Preprint arXiv:2307.10802.\\n\\nFootnotes\\n\\nExample adapted from Deep Learning with Python by Francois Chollet↩︎\\nIn the transformer paper, MLPs are what referred to as feed-forward networks(FFNs). I find the terminology of FFNs confusing sometime. MLPs are feed-forward networks but not the other way around.↩︎\\nIf you want to see how embeddings look like and how words with same semantic meaning tend to be closer to each other, you can play with Embedding Projector↩︎\\nThe core operation in attention is the dot product between query and keys, which, being a summation operation, is permutation invariant↩︎\\nHat tip to Sebastian Raschka for sharing this in his newsletter↩︎\\nBertViz be accessed at https://github.com/jessevig/bertviz↩︎\\nKarpathy said that in a Twitter thread. Available here: https://twitter.com/karpathy/status/1655994367033884672?s=20↩︎\\n Next sentence prediction in BERT and next token prediction in standard transformer are different. The idea is roughly similar, but the former is usually for discriminative modelling while the later is for auto-regressive generative modelling↩︎\\nNext token prediction in decoder LLMs is different to next sentence prediction in BERT. The former operates on token level while the later operates on sentence level↩︎\\nIt’s fair to say that GPT-3 popularized prompt engineering.↩︎\\nThe inductive biases in ConvNets are the results of their translation invariance. Convolution itself is translation equivariance(changing the position of pixels changes the output) but pooling which is often used after convolution is translation invariant(changing the position of pixels doesn’t change the output) and this make the overall ConvNets translation invariant architecture↩︎\\nGPU main memory is called HBM which stands for High Bandwidth Memory↩︎\\nAvailable at https://github.com/tensorflow/tensor2tensor↩︎\\nAvailable at https://github.com/google/trax↩︎\\n\\nCitationBibTeX citation:@article{nyandwi2023,\\n  author = {Nyandwi, Jean},\\n  title = {The {Transformer} {Blueprint:} {A} {Holistic} {Guide} to the\\n    {Transformer} {Neural} {Network} {Architecture}},\\n  journal = {Deep Learning Revision},\\n  date = {2023-07-29},\\n  url = {https://deeprevision.github.io/posts/001-transformer/},\\n  langid = {en}\\n}\\nFor attribution, please cite this work as:\\nNyandwi, Jean. 2023. “The Transformer Blueprint: A Holistic Guide\\nto the Transformer Neural Network Architecture.” Deep\\nLearning Revision, July. https://deeprevision.github.io/posts/001-transformer/.\\n \\n\\n \\n'),\n",
              " Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nnlp\\nlanguage-model\\nagent\\nsteerability\\nprompting\\n\\n\\n\\n« \\n\\nAdversarial Attacks on LLMs\\n\\n\\n »\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil\\'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
              " Document(metadata={'description': 'Large language model (LLM) inference is a full-stack challenge. Powerful GPUs, high-bandwidth GPU-to-GPU interconnects, efficient acceleration libraries…', 'language': 'en-US', 'source': 'https://developer.nvidia.com/blog/nvidia-blackwell-platform-sets-new-llm-inference-records-in-mlperf-inference-v4-1/', 'title': 'NVIDIA Blackwell Platform Sets New LLM Inference Records in MLPerf Inference v4.1 | NVIDIA Technical Blog'}, page_content=\"\\n\\n\\n\\n\\n\\n\\nNVIDIA Blackwell Platform Sets New LLM Inference Records in MLPerf Inference v4.1 | NVIDIA Technical Blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDEVELOPER\\n\\n\\n\\nHomeBlogForumsDocsDownloadsTraining\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\nJoin\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Related Resources \\n\\n\\n\\n\\n\\n\\n\\n\\nData Center / Cloud\\n\\n\\n\\nEnglish中文\\n\\n\\nNVIDIA Blackwell Platform Sets New LLM Inference Records in MLPerf Inference v4.1\\n\\n\\n            Aug 28, 2024\\n          \\n\\n            By Ashraf Eassa, Ashwin Nanjappa, Zhihan Jiang, Yiheng Zhang, Jun Yang, Zihao Kong and Shengliang Xu \\n\\n\\n  \\n\\n                Like              \\n\\n\\n\\n Discuss (1)\\n                \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nL\\nT\\nF\\nR\\nE\\n \\n\\nLarge language model (LLM) inference is a full-stack challenge. Powerful GPUs, high-bandwidth GPU-to-GPU interconnects, efficient acceleration libraries, and a highly optimized inference engine are required for high-throughput, low-latency inference.\\xa0\\nMLPerf Inference v4.1 is the latest version of the popular and widely recognized MLPerf Inference benchmarks, developed by the MLCommons consortium. The benchmark includes many popular AI models covering diverse use cases, from LLMs and generative AI to recommenders and computer vision. The benchmarks are regularly updated to ensure market relevance.\\xa0\\nIn this round, NVIDIA submitted many great results, enabled by innovation across the NVIDIA technology stack. Highlights include:\\xa0\\n\\nFirst submission using the NVIDIA Blackwell architecture, delivering up to 4x more performance on Llama 2 70B compared to the NVIDIA H100 Tensor Core GPU.\\nNVIDIA H200 Tensor Core GPU submissions on every data center workload, delivering up to 1.5x more performance compared to the H100 submissions.\\nUp to 27% more performance on H200 due to software improvements compared to preview submissions on H200 made in the prior round.\\xa0\\nFirst Llama 2 70B submissions using NVIDIA Triton Inference Server, delivering similar performance to NVIDIA TensorRT-LLM submissions.\\xa0\\nUp to 6.2x higher performance on the GPT-J benchmark in the edge category compared to the prior round using the NVIDIA Jetson AGX Orin platform.\\n\\nThis post provides a closer look at these results.\\xa0\\nNVIDIA Blackwell shines in MLPerf Inference debut\\nIntroduced at NVIDIA GTC 2024, the NVIDIA Blackwell architecture is a new class of AI superchip. Crafted with 208 billion transistors, and using the TSMC 4NP process tailored for NVIDIA, it is the largest GPU ever built. The Blackwell architecture also features the new second-generation Transformer Engine, which uses new Blackwell Tensor Core technology combined with TensorRT-LLM innovations, to enable fast and accurate FP4 AI inference.\\xa0\\xa0\\n\\nFigure 1. An NVIDIA Blackwell compute node\\xa0\\nIn this round of MLPerf Inference, NVIDIA made its first submissions using Blackwell. On the Llama 2 70B LLM benchmark, Blackwell delivered up to 4x higher tokens per second per GPU compared to the H100 GPU.\\nMLPerf Inference v4.1\\xa0Llama 2 70BServertokens/sOfflinetokens/s1 NVIDIA B200 GPU10,75611,264Per-GPU increase4x3.7xTable 1. Per-GPU performance increases compared to NVIDIA Hopper on the MLPerf Llama 2 70B benchmark. H100 per-GPU throughput obtained by dividing submitted eight-GPU results by eight\\nMLPerf Inference\\xa0 v4.1 Closed, Data Center. Results retrieved from www.mlperf.org on August 28, 2024. Blackwell results measured on single GPU and retrieved from entry 4.1-0074 in the Closed, Preview category. H100 results from entry 4.1-0043 in the Closed, Available category on eight\\xa0 H100 system and divided by GPU count for per GPU comparison. Per-GPU throughput is not a primary metric of MLPerf Inference. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is\\xa0 strictly prohibited. See www.mlcommons.org for more information.\\nThis submission made extensive use of the Blackwell FP4 Transformer Engine. This submission was also in the Closed division, meaning that the inference results delivered this performance without modifications to the model while still meeting the high accuracy requirements of the benchmark. FP4 quantization was performed using the NVIDIA TensorRT Model Optimizer library, which incorporates state-of-the-art model optimization techniques, and did not require model re-training.\\xa0\\nNVIDIA H200 Tensor Core GPU delivers outstanding performance on every benchmark\\nThe NVIDIA H200 GPU upgrades the NVIDIA Hopper architecture with HBM3e, the industry’s fastest AI memory. Compared to the H100, this increases memory capacity by 1.8x and memory bandwidth by 1.4x, benefiting memory-sensitive use cases.\\nThis round, NVIDIA submitted results using eight H200 GPUs on every workload, and did so in the available category.\\xa0\\nBenchmarkGPUServerOfflineLlama 2 70B\\xa08 H200(1,000 W)32,790\\xa0token/s34,864\\xa0token/sMixtral 8x7B8 H200(700 W)57,177\\xa0token/s59,022\\xa0token/sGPT-J19,243\\xa0token/s20,086\\xa0token/sStable Diffusion XL16.78\\xa0queries/s17.42\\xa0samples/sDLRM v2 99%585,208\\xa0queries/s637,342samples/sDLRM v2 99.9%370,083\\xa0queries/s390,953samples/sResNet-50 v1.5632,229\\xa0queries/s756,960samples/sBERT 99%57,609\\xa0queries/s73,310\\xa0samples/sBERT 99.9%51,212\\xa0queries/s63,950\\xa0samples/sRetinaNet13,604\\xa0queries/s14,439\\xa0samples/s3D U-NetNot part of benchmark54.71\\xa0samples/sTable 2.\\xa0NVIDIA MLPerf Inference v4.1 data center results using H200 GPUs. Llama 2 70B results based on H200 configured at 1000W, all other results using H200 at 700W\\xa0\\nMLPerf Inference v4.1 Closed, Data Center. Results retrieved from www.mlperf.org on August 28, 2024. All results using eight GPUs and retrieved from the following entries: 4.1-0046, 4.1-0048, 4.1-0050. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. See www.mlcommons.org for more information.\\nThe following subsections describe the improvements achieved across several benchmarks.\\nLlama 2 70B\\nThe Llama 2 70B benchmark was first introduced in the prior round and continues to represent popular, 70B-class dense LLMs.\\nNVIDIA also continues to enhance TensorRT-LLM software, providing users with more LLM inference performance from the GPUs they already have. Through software improvements alone, Llama 2 70B performance improved by up to 14% on H200 compared to the preview submission in the prior round.\\xa0\\nMLPerf Llama 2 70B improvements since v4.0ServerOfflineH200 (700 W)1.14x1.12xH100 (700 W)1.05x1.12xTable 3.\\xa0Hopper GPU improvements on Llama 2 70B benchmark compared to prior round\\xa0\\xa0\\xa0\\nMLPerf Inference v4.0 and\\xa0 v4.1 Closed, Data Center. Results retrieved from www.mlperf.org on August 28, 2024. All results using eight GPUs and retrieved from the following entries:4.0-0062, 4.0-0070\\xa0 4.1-0043, 4.1-0048, 4.1-0050. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. See www.mlcommons.org for more information.\\nKey improvements this round included XQA kernel optimizations as well as additional layer fusions.\\xa0\\nAdditionally, NVIDIA submitted Llama 2 70B results using H200 GPUs using a custom thermal solution and with the thermal design power (TDP) increased to 1,000 watts. This enabled an additional performance increase of up to 12% on the Llama 2 70B benchmark, compared to H200 configured at a 700-watt TDP.\\xa0\\nThis round, NVIDIA also submitted Llama 2 70B results using H200 GPUs running Triton Inference Server, delivering similar performance to the bare metal submission. In the server scenario, H200 with Triton Inference Server even outperformed H200 without Triton Inference Server.\\xa0\\nMLPerf Llama 2 70B benchmarkServerOffline8 H200 with Triton Inference Server30,12831,0598 H200 without Triton Inference Server29,22831,303Table 4.\\xa0Performance of eight H200 GPUs with and without Triton Inference Server\\nMLPerf Inference v4.1 Closed, Data Center. Results retrieved from www.mlperf.org on August 28, 2024. All results using eight GPUs and retrieved from the following entries :4.1-0048, 4.1-0050. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. See www.mlcommons.org for more information.\\nThis shows that organizations looking to deploy popular models need not trade functionality for performance when using Triton Inference Server.\\nAnd, finally, NVIDIA submitted Llama 2 70B results in the Open division using a single H200 GPU, showcasing the possible performance gains that can result from more extensive model-level optimizations.\\xa0\\nFirst, depth pruning and width pruning were applied to the model, greatly reducing the total number of parameters in the model by intelligently removing layers and MLP intermediate dimensions that are less important to the overall model output.\\xa0\\nThen, to recover accuracy, fine-tuning was performed on the model using the MLPerf OpenORCA development dataset. The final pruned model has 32 layers and 14,336 MLP intermediate dimensions—a significant reduction compared to the original model’s 80 layers and 28,672 intermediate dimensions.\\xa0\\nAlthough model accuracy is slightly below the 99% threshold, the model is significantly smaller, enabling much higher throughput (offline) of 11,189 token/s—or almost 3x higher throughput compared to the throughput achieved in the Closed division.\\xa0\\xa0\\nMLPerf Inference v4.1, Data Center, Open Division. Result from entry 4.1-0089. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. See www.mlcommons.org for more information.\\nMixtral 8x7B\\nA new LLM workload was added in this round, based on the Mixtral 8x7B model, developed by Mistral AI. Mixtral 8x7B employs a sparse mixture of experts (MoE) architecture with eight experts, 46.7B total parameters, with two experts and 12.9B parameters used per token.\\xa0\\nNVIDIA submitted Mixtral 8x7B results using both the H100 and H200 GPUs, running TensorRT-LLM software and extensively used FP8 precision.\\xa0\\nMLPerf Mixtral 8x7B benchmarkServertokens/sOfflinetokens/s8 H20057,17759,0228 H10050,79652,416H200 advantage1.13x1.13xTable 5. H100 and H200 performance and uplift for the latter on MLPerf Mixtral 8x7B benchmark\\xa0\\nMLPerf Inference v4.1 Closed, Data Center. Results retrieved from www.mlperf.org on August 28, 2024. All results using eight GPUs and retrieved from the following entries :4.1-0043, 4.1-0048. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. See www.mlcommons.org for more information.\\nOnly systems using NVIDIA GPUs submitted Mixtral 8x7B results. NVIDIA continues to submit great results on workloads as they are added to the MLPerf benchmark suite, showing that the NVIDIA platform delivers high performance and exceptional versatility for the large and expanding universe of AI models.\\xa0\\nStable Diffusion XL\\nThis round, H200 performance was improved to generate two images per second, which is 27% more performance on Stable Diffusion XL compared to the prior round. This represented a new record for the benchmark.\\xa0\\nMLPerf Stable Diffusion XL improvements since v4.0ServerOffline8 H200 (700 W)1.22x1.27x8 H100 (700 W)1.17x1.25xTable 6. Stable Diffusion XL performance increases in MLPerf Inference v4.1 compared to v4.0 on both H100 and H200 GPUs\\xa0\\nMLPerf Inference v4.0 and v4.1 Closed, Data Center. Results retrieved from www.mlperf.org on August 28, 2024. All results using eight GPUs and retrieved from the following entries :4.0-0062, 4.0-0070, 4.1-0043, 4.1-0048. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. See www.mlcommons.org for more information.\\nThese gains were due primarily to several key optimizations to the NVIDIA software stack, including:\\n\\nUNet FP8 support: By using TensorRT Model Optimizer, the NVIDIA submission this round used FP8 precision while meeting accuracy requirements. This represented the largest portion of the round-to-round performance gain on Hopper GPUs.\\xa0\\nVAE INT8 support: The NVIDIA submission this round was able to quantize certain layers to INT8 and others to FP16, compared to use of FP32 in the prior round. This improved VAE performance by 70%, translating into about a 4% end-to-end speedup.\\xa0\\n\\nVariational autoencoder (VAE) batch splitting: The VAE portion of the SDXL pipeline requires a very large memory footprint. By employing batch splitting, the NVIDIA submission this round was able to increase batch size from 8 to 64, improving performance.\\xa0\\n\\nFigure 2. Stable Diffusion XL model architecture\\nAdditionally, NVIDIA submitted SDXL results in the Open division submission, which incorporates these optimizations with Latent Consistency Model (LCM), accelerating the Closed division offline throughput by almost 5x to 11 samples/s on H200. This showcased the further performance gains possible from more extensive model-level optimizations for diffusion models.\\nA giant generative AI leap on Jetson AGX Orin\\nJetson AGX Orin offers high AI compute performance, large unified memory, and comprehensive software for generative AI at the edge.\\xa0\\nThrough extensive software optimization, NVIDIA Jetson AGX Orin 64 GB delivers a large giant leap for generative AI models for the edge, delivering up to 6.2x more throughput and 2.4x better latency on the GPT-J 6B parameter LLM benchmark. Generative AI models at the edge can transform sensor data, such as images and videos, into real-time, actionable insights with strong contextual awareness.\\xa0\\nBacked by the NVIDIA software stack, Jetson AGX Orin is uniquely positioned as the leading platform for running transformer models like GPT-J, vision transformers, and Stable Diffusion at the Edge. Developers can take advantage of other platform services like Jetson Generative AI Lab and Jetson Platform Services to bring great solutions to life.\\xa0\\nGPT-J (Edge)Single stream latency (ms)Offline tokens/sJetson AGX Orin 64 GB v4.14,17664.47Jetson AGX Orin 64 GB\\xa0 v4.010,13210.35Table 7. GPT-J LLM performance in the MLPerf Inference; Edge (v4.0 and v4.1) on Jetson AGX Orin\\nMLPerf Inference v4.0 and v4.1 Closed, Edge. Results retrieved from www.mlperf.org on August 28, 2024. All results using eight GPUs and retrieved from the following entries :4.0-0072, 4.1-0051. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. See www.mlcommons.org for more information.\\nThis performance boost was made possible through numerous software optimizations to TensorRT-LLM, including the use of in-flight batching, as well as the application of INT4 Activation-aware Weight Quantization (AWQ). AWQ keeps the 1% “salient weights” in higher precision FP16 and quantizes the remaining weights to four-bit integer (INT4) precision. This technique significantly reduces memory footprints, enabling larger batches to be processed at once, dramatically increasing inference throughput.\\xa0\\nNVIDIA also submitted results of the demanding Llama 2 70B model running on Jetson AGX Orin in the Open division, demonstrating the possibilities of more extensive model optimization techniques. The submission model was the same 16B depth and width pruned model as in the H200 submission. INT4 AWQ—used in the GPT-J submission on Jetson AGX Orin in the Closed division—was also used in this submission. Model parameter pruning plus INT4 quantization together greatly shrink the memory footprint of the model weights to around only 8 GB for the Llama 2 70B model.\\nConclusion\\nIn its debut submission, NVIDIA Blackwell delivered outstanding performance—up to 4x compared to H100 on Llama 2 70B. And, among available solutions, Hopper GPUs delivered the highest multi-GPU generative AI performance and the highest performance per accelerator across all workloads, and continues to benefit from ongoing software optimization. NVIDIA Triton Inference Server also achieved great results this round, delivering similar performance to bare metal submissions. For edge and embedded AI, Jetson AGX Orin, and the rich NVIDIA software stack enable running capable models, like GPT-J 6B, with performance improving by up to 6.2x in just one round.\\xa0\\nNVIDIA continues to innovate rapidly across the full technology stack to deliver world-class inference performance on today’s models as well as tomorrow’s, from the largest AI factories to compact, low-power edge devices.\\nAcknowledgments\\nThe work of many NVIDIA employees made these outstanding results happen. We would like to acknowledge the tireless efforts of Chen-Han Yu, Kai Xu, Justin Xin, Asma Kuriparambil Thekkumpate, Linnan Wang, Wei-Ming Chen Kaiyu Xie, Shobhit Verma, Viraat Chandra, among many others.\\n\\n\\nRelated resources\\nGTC session: LLM Inference Sizing: Benchmarking End-to-End Inference SystemsGTC session: Optimizing and Scaling LLMs With TensorRT-LLM for Text GenerationGTC session: Speeding up LLM Inference With TensorRT-LLMNGC Containers: NVIDIA MLPerf InferenceNGC Containers: NVIDIA MLPerf InferenceSDK: Triton Management Service\\n\\n\\n\\n\\n Discuss (1)\\n        \\n\\n\\n  \\n\\n      Like    \\n\\n\\n\\n\\nTags\\n\\n\\nData Center / Cloud | Generative AI / LLMs | Hardware / Semiconductor | Blackwell | H100 | Hopper | Jetson | TensorRT | TensorRT-LLM | Triton Inference Server | Intermediate Technical | Benchmark | featured | LLMs | MLPerf \\n\\n\\n\\n\\n\\n                About the Authors\\n          \\n\\n\\n\\n\\n\\n\\n\\n                  About Ashraf Eassa\\n                \\n\\n                \\n                          Ashraf Eassa is a senior product marketing manager at NVIDIA, focusing on deep learning, training and inference. He holds bachelor's degrees in computer science and mathematics from the University of Vermont.\\n            \\n                                                    \\n                          \\n\\n                                            View all posts by Ashraf Eassa\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                  About Ashwin Nanjappa\\n                \\n\\n                \\n                          Ashwin Nanjappa is an engineering manager in the TensorRT team at NVIDIA. He leads the MLPerf Inference initiative to demonstrate the performance and energy efficiency of NVIDIA accelerators. He is also involved in improving the DL inference performance of the TensorRT library. Before joining NVIDIA, he worked on training DL models for CV, GPU-accelerated ML algorithms for depth cameras, and developing multimedia libraries for cellphones and DVD players. He has a Ph.D. in computer science from the National University of Singapore (NUS), with a focus on GPU algorithms for 3D computational geometry.\\n            \\n                                                    \\n                          \\n\\n                                            View all posts by Ashwin Nanjappa\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                  About Zhihan Jiang\\n                \\n\\n                \\n                          Zhihan Jiang is a senior system software engineer on the TensorRT team at NVIDIA and focuses on delivering world-class inference results in MLPerf Inference. Before working on MLPerf, he worked on TensorRT autonomous safety libraries and infrastructure, and NVIDIA CPU architecture modeling. Zhihan holds an M.S. degree in electrical engineering from Stanford university, and a B.S. degree in computer engineering from Georgia Tech.\\n            \\n                                                    \\n                          \\n\\n                                            View all posts by Zhihan Jiang\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                  About Yiheng Zhang\\n                \\n\\n                \\n                          Yiheng Zhang is a Software Engineer on the TensorRT team at NVIDIA, where he focuses on MLPerf Inference. Yiheng has experience with autonomous driving software, Jetson platform software optimization, and general performance optimization in MLPerf Inference. Yiheng holds a master's degree in Computer Science from Stanford University.\\n            \\n                                                    \\n                          \\n\\n                                            View all posts by Yiheng Zhang\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                  About Jun Yang\\n                \\n\\n                \\n                          Jun Yang is a senior engineering director at NVIDIA, where he focuses on E2E AI workload optimization. Currently, he is leading the overall engineering efforts of NVIDIA TensorRT-LLM.  He holds a master’s degree in Computer Architecture from the Institute of Computing Technology Chinese Academy of Sciences.\\n            \\n                                                    \\n                          \\n\\n                                            View all posts by Jun Yang\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                  About Zihao Kong\\n                \\n\\n                \\n                          Zihao Kong is a software engineer on the TensorRT team at NVIDIA. He focuses on delivering world-class deep learning inference performance on NVIDIA data center GPUs and Jetson platform GPUs on edge. He has experience with performance analysis and profiling and deep learning accelerators as well. He holds a bachelor’s degree in Computer Engineering from UC San Diego.\\n            \\n                                                    \\n                          \\n\\n                                            View all posts by Zihao Kong\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                  About Shengliang Xu\\n                \\n\\n                \\n                          Shengliang Xu is a senior deep learning engineer on the NVIDIA Algorithmic Model Optimization team focused on end-to-end optimization of deep learning model inference on NVIDIA GPU platforms. His research and development interests span model and inference system optimization of large language models and large generative models. Shengliang holds an M.S. degree in computer science from University of Washington, where he dropped out the Ph.D. program. He holds another M.S. degree and a B.S. degree both in computer science from Shanghai Jiao Tong University.\\n            \\n                                                    \\n                          \\n\\n                                            View all posts by Shengliang Xu\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRelated posts\\n\\n\\n\\n\\n\\n\\n\\n\\n          Supercharging Llama 3.1 across NVIDIA Platforms\\n        \\n\\n\\n\\nSupercharging Llama 3.1 across NVIDIA Platforms\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Achieving High Mixtral 8x7B Performance with NVIDIA H100 Tensor Core GPUs and NVIDIA TensorRT-LLM\\n        \\n\\n\\n\\nAchieving High Mixtral 8x7B Performance with NVIDIA H100 Tensor Core GPUs and NVIDIA TensorRT-LLM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          NVIDIA H200 Tensor Core GPUs and NVIDIA TensorRT-LLM Set MLPerf LLM Inference Records\\n        \\n\\n\\n\\nNVIDIA H200 Tensor Core GPUs and NVIDIA TensorRT-LLM Set MLPerf LLM Inference Records\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          NVIDIA TensorRT-LLM Enhancements Deliver Massive Large Language Model Speedups on NVIDIA H200\\n        \\n\\n\\n\\nNVIDIA TensorRT-LLM Enhancements Deliver Massive Large Language Model Speedups on NVIDIA H200\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          NVIDIA TensorRT-LLM Supercharges Large Language Model Inference on NVIDIA H100 GPUs\\n        \\n\\n\\n\\nNVIDIA TensorRT-LLM Supercharges Large Language Model Inference on NVIDIA H100 GPUs\\n\\n\\n\\n\\n\\nRelated posts\\n\\n\\n\\n\\n\\n\\n\\n\\n          NVIDIA GH200 Grace Hopper Superchip Delivers Outstanding Performance in MLPerf Inference v4.1\\n        \\n\\n\\n\\nNVIDIA GH200 Grace Hopper Superchip Delivers Outstanding Performance in MLPerf Inference v4.1\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          NVIDIA Sets New Generative AI Performance and Scale Records in MLPerf Training v4.0\\n        \\n\\n\\n\\nNVIDIA Sets New Generative AI Performance and Scale Records in MLPerf Training v4.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          NVIDIA H200 Tensor Core GPUs and NVIDIA TensorRT-LLM Set MLPerf LLM Inference Records\\n        \\n\\n\\n\\nNVIDIA H200 Tensor Core GPUs and NVIDIA TensorRT-LLM Set MLPerf LLM Inference Records\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Optimizing OpenFold Training for Drug Discovery\\n        \\n\\n\\n\\nOptimizing OpenFold Training for Drug Discovery\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Setting New Records at Data Center Scale Using NVIDIA H100 GPUs and NVIDIA Quantum-2 InfiniBand\\n        \\n\\n\\n\\nSetting New Records at Data Center Scale Using NVIDIA H100 GPUs and NVIDIA Quantum-2 InfiniBand\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nL\\nT\\nF\\nR\\nE\\n \\n\\n\\n\")]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Router function in LangGraph application"
      ],
      "metadata": {
        "id": "1hkV8ReilHKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ8D34ywlLPu",
        "outputId": "98c52f64-483a-4a14-82cc-6251771af5e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use the BaseModel"
      ],
      "metadata": {
        "id": "dut8tUcYlcDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data model\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"vectorstore\", \"wiki_search\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose to route it to wikipedia or a vectorstore.\",\n",
        "    )"
      ],
      "metadata": {
        "id": "BSRlADCcktf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GroQ LLM and setup"
      ],
      "metadata": {
        "id": "rwM2vvH9mbkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "# LLM with function call\n",
        "from langchain_groq import ChatGroq\n",
        "import os\n",
        "\n",
        "groq_api_key=userdata.get('llama_3.1_groq')\n",
        "os.environ[\"GROQ_API_KEY\"]=groq_api_key\n",
        "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Llama-3.1-70b-Versatile\")"
      ],
      "metadata": {
        "id": "Qem6AbJpmgBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KijTf2BJnIoR",
        "outputId": "2ed39884-7ddc-474f-bce4-024f4a3acceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7940d82126e0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7940d8211720>, model_name='Llama-3.1-70b-Versatile', model_kwargs={}, groq_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# integrate the Route query with LLM"
      ],
      "metadata": {
        "id": "k6iqkSGInOaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm_router = llm.with_structured_output(RouteQuery)"
      ],
      "metadata": {
        "id": "FarfgkW1nNy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "system = \"\"\"You are an expert at routing a user question to a vectorstore or wikipedia.\n",
        "The vectorstore contains documents related to deep learning transformer architecture,ai agent, and Nvidia MLPerf Inference v4.1 .\n",
        "Use the vectorstore for questions on these topics. Otherwise, use wiki-search.\"\"\"\n",
        "route_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "TPAgYmdPni-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_router = route_prompt | structured_llm_router\n",
        "print(\n",
        "    question_router.invoke(\n",
        "        {\"question\": \"Who is AIRBUS?\"}\n",
        "    )\n",
        ")\n",
        "print(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuwgQJw5oHAq",
        "outputId": "6e2733e5-5b68-4124-f578-f776cb470267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasource='wiki_search'\n",
            "datasource='vectorstore'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create WikiSearch"
      ],
      "metadata": {
        "id": "AoBJnszRoiCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community\n",
        "!pip install arxiv wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUtq-XJUokz0",
        "outputId": "fcc10ce3-922a-48f5-e78d-b01dea391ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.1)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.129)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.5.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.1->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.1->langchain_community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.7)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain_community) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.2)\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wikipedia, sgmllib3k\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=8accf2ccd14c77f8970134eefe61dfeca93575f7de08f016b14d819fb27883fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=0aa69c79d3d3d03a65b8d7ffcfd55a332c44d39d918a52cb2b260957dec2344d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built wikipedia sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, wikipedia, arxiv\n",
            "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0 wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities import ArxivAPIWrapper,WikipediaAPIWrapper\n",
        "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun"
      ],
      "metadata": {
        "id": "MdQfw3JGpTo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Arxiv (not using in this blog ) and wikipedia Tools\n",
        "# arxiv_wrapper=ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
        "# arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
        "\n",
        "api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=200)\n",
        "wiki=WikipediaQueryRun(api_wrapper=api_wrapper)"
      ],
      "metadata": {
        "id": "HQKzSo-Lo-Am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki.run(\"PANASONIC\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Cc1sG94-pXr1",
        "outputId": "f696d661-bdbe-431c-da35-6e6f59ce7be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Page: Panasonic\\nSummary: Panasonic Holdings Corporation is a Japanese multinational electronics company, headquartered in Kadoma, Osaka, Japan. It was founded in 1918 as Matsushita Electric Housewares'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Agent Application using LangGraph"
      ],
      "metadata": {
        "id": "d5LsC3N2p4EJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define Graph State with three information as part of the state.\n",
        "\n",
        "from typing import List\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    generation: str\n",
        "    documents: List[str]"
      ],
      "metadata": {
        "id": "OlafbsWVpzUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reteriver in sync with State as question,& returns documents and question. Refer the variable of the state function above"
      ],
      "metadata": {
        "id": "bhDwGBMsq0Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval of data\n",
        "    documents = retriever.invoke(question)\n",
        "    return {\"documents\": documents, \"question\": question}"
      ],
      "metadata": {
        "id": "jaq0yl_5q9Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WikiSearch similar to reteriver. Function is to do wikipedia search and return the documents"
      ],
      "metadata": {
        "id": "MBTQcAHjrlu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "def wiki_search(state):\n",
        "    \"\"\"\n",
        "    wiki search based on the re-phrased question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with appended web results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---wikipedia---\")\n",
        "    print(\"---HELLO--\")\n",
        "    question = state[\"question\"]\n",
        "    print(question)\n",
        "\n",
        "    # Wiki search\n",
        "    docs = wiki.invoke({\"query\": question})\n",
        "    #print(docs[\"summary\"])\n",
        "    wiki_results = docs\n",
        "    wiki_results = Document(page_content=wiki_results)\n",
        "\n",
        "    return {\"documents\": wiki_results, \"question\": question}"
      ],
      "metadata": {
        "id": "W9eymUJdrott"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define the Router as a function also, its function is to decide whether wiki search or vectorstore then return the document respectively"
      ],
      "metadata": {
        "id": "wC1PD7qtsB4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def route_question(state):\n",
        "    \"\"\"\n",
        "    Route question to wiki search or RAG.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ROUTE QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    source = question_router.invoke({\"question\": question})\n",
        "    if source.datasource == \"wiki_search\":\n",
        "        print(\"---ROUTE QUESTION TO Wiki SEARCH---\")\n",
        "        return \"wiki_search\"\n",
        "    elif source.datasource == \"vectorstore\":\n",
        "        print(\"---ROUTE QUESTION TO RAG---\")\n",
        "        return \"vectorstore\""
      ],
      "metadata": {
        "id": "S3kyLJkzsQLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Graph"
      ],
      "metadata": {
        "id": "A6ZX5Je-smHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "# Define the nodes\n",
        "workflow.add_node(\"wiki_search\", wiki_search)  # web search\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "\n",
        "# Build graph\n",
        "workflow.add_conditional_edges(\n",
        "    START,\n",
        "    route_question,\n",
        "    {\n",
        "        \"wiki_search\": \"wiki_search\",\n",
        "        \"vectorstore\": \"retrieve\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge( \"retrieve\", END)\n",
        "workflow.add_edge( \"wiki_search\", END)\n",
        "# Compile\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "cLwfVcd6spXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(app.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "NrMCETCCsvBv",
        "outputId": "c343d3cb-9ba8-4755-b780-0cc0147bef97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Test Inference"
      ],
      "metadata": {
        "id": "-us7voqds7fW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\n",
        "    \"question\": \"What is agent?\"\n",
        "}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value['documents'][0].dict()['metadata']['description'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-_fEVDUs-sb",
        "outputId": "9c4d811d-edd6-41e8-a2c2-d26689b47f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---ROUTE QUESTION---\n",
            "---ROUTE QUESTION TO RAG---\n",
            "---RETRIEVE---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "('Building agents with LLM (large language model) as its core controller is a '\n",
            " 'cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer '\n",
            " 'and BabyAGI, serve as inspiring examples. The potentiality of LLM extends '\n",
            " 'beyond generating well-written copies, stories, essays and programs; it can '\n",
            " 'be framed as a powerful general problem solver.\\n'\n",
            " 'Agent System Overview In a LLM-powered autonomous agent system, LLM '\n",
            " 'functions as the agent’s brain, complemented by several key components:')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "inputs = {\n",
        "    \"question\": \"locomotive\"\n",
        "}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value['documents'][0].dict()['metadata']['description'])"
      ],
      "metadata": {
        "id": "tFnwvrsztKtV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}